# Vision encoder: vit | clip
VISION_ENCODER=clip

# Top-K Qwen blocks to unfreeze (0 = fully frozen)
TOP_K=3

# Vision encoder model name: flickr30k, flation, fashion_mnist
TRAINING_DATASET=flickr30k

BRIDGE_CROSS_ENTROPY=True
BATCH_SAMPLING=100

# Caption processing settings
SPLIT_CAPTIONS=False

# Core hyper-parameters (tweak as needed)
BATCH_SIZE=16
EPOCHS=3
LR_ADAPTER=1e-4
LR_QWEN=2e-5
SEED=42

# Logging / storage
OUTPUT_DIR_MODELS=../.data/models
OUTPUT_DIR_DATASETS=../.data/datasets
OUTPUT_DIR_CACHE=../.data/cache

WANDB_API_KEY=...
WANDB_ENTITY=charles-cai
WANDB_PROJECT=mlx8-w4-multimodal-transferlearning

# HF private token (optional)
HF_TOKEN=...

# Inference settings
INFERENCE_PROMPT=Examine the image carefully and provide a detailed description covering objects, people, actions, background, and any notable visual elements

# Training prompt settings - better alternatives to SOS/EOS only
USE_PROMPT_VARIATION=True
PROMPT_DROPOUT_RATE=0.15
PROMPT_RANDOM_SELECTION=True
PROMPT_SHUFFLE_PER_EPOCH=True
USE_SHORTENED_PROMPTS=True
MIN_PROMPT_LENGTH=3

# training settings, not used for now, but can test faster training with limited datasets for training
LIMITING_FACTOR=1           #Training data (80%)
LIMITING_FACTOR_EVAL=0.1    # Evaluation data -> per epoch, using eval dataset(10%)
LIMITING_FACTOR_TEST=0.1    # Test data -> post training, using test dataset (10%)

SPLIT_CAPTIONS=False